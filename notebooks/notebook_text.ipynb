{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c651616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text Generation Pipeline with GPT2 on Wikitext-2 (Fixed Version)\n",
    "\n",
    "# # --- 1. Imports ---\n",
    "# import torch\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments, TrainerCallback\n",
    "# from datasets import Dataset\n",
    "# import logging\n",
    "\n",
    "# # --- 2. Setup ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- 3. Load Pretrained Model and Tokenizer ---\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# # Define a padding token (GPT-2 does not have one by default)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Move the model to the appropriate device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Enable gradient checkpointing to save memory\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# # --- 4. Use Predefined Dataset ---\n",
    "# # Simulated high-quality text dataset to ensure stability\n",
    "# train_texts = [\n",
    "#     \"Once upon a time, in a land far away, there lived a wise old king.\",\n",
    "#     \"The quick brown fox jumps over the lazy dog.\",\n",
    "#     \"Artificial intelligence is transforming the world in many ways.\",\n",
    "#     \"The rain in Spain stays mainly in the plain.\",\n",
    "#     \"In the midst of chaos, there is also opportunity.\"\n",
    "# ] * 1000  # Repeat to simulate dataset size\n",
    "\n",
    "# train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "# # --- 5. Prepare Dataset ---\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=False\n",
    "# )\n",
    "\n",
    "# # --- 6. Training Arguments ---\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./textgen_model\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     save_steps=500,\n",
    "#     save_total_limit=2,\n",
    "#     prediction_loss_only=True,\n",
    "#     logging_steps=100,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     report_to=\"none\",\n",
    "#     max_grad_norm=1.0\n",
    "# )\n",
    "\n",
    "# # --- 7. Trainer ---\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tokenized_dataset\n",
    "# )\n",
    "\n",
    "# # Add a callback to log training loss\n",
    "# class LogCallback(TrainerCallback):\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         if logs is not None:\n",
    "#             logger.info(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "# trainer.add_callback(LogCallback())\n",
    "\n",
    "# # --- 8. Train ---\n",
    "# trainer.train()\n",
    "\n",
    "# # --- 9. Test Text Generation ---\n",
    "# def generate_text(prompt, max_length=100):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         max_length=max_length,\n",
    "#         num_return_sequences=1,\n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         top_p=0.95\n",
    "#     )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # --- 10. Example Usage ---\n",
    "# prompt = \"The future of artificial intelligence\"\n",
    "# print(\"\\n--- Generated Text ---\\n\")\n",
    "# print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d669d42",
   "metadata": {},
   "source": [
    "# Script d'entraînement GPT-2 avec LoRA sur Wikitext-2\n",
    "Ce notebook structure le script en cellules pour faciliter la compréhension et l'exécution.\n",
    "\n",
    "## 1. Importation des bibliothèques nécessaires\n",
    "On importe les modules essentiels pour le traitement des données, le modèle, l'entraînement et la gestion des configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a9811b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import notebook_login\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df67540",
   "metadata": {},
   "source": [
    "## 2. Définition des configurations\n",
    "On définit les noms des modèles, les chemins de sauvegarde, les hyperparamètres d'entraînement et les paramètres LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d093584",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilgpt2\"\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_CONFIG = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 128,\n",
    "    \"warmup_steps\": 100,\n",
    "}\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"c_attn\", \"c_proj\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871be77",
   "metadata": {},
   "source": [
    "## 3. Fonction principale\n",
    "Définition de `main()` pour organiser le flux d'exécution de bout en bout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c4b6839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'learning_rate': 0.0003, 'num_epochs': 3, 'max_length': 128, 'warmup_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_CONFIG)\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    accelerator = Accelerator()\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Chargement du dataset...\")\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "\n",
    "    print(\"Chargement du tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Prétraitement des données...\")\n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": TRAIN_CONFIG[\"max_length\"]},\n",
    "    )\n",
    "\n",
    "    print(\"Initialisation du modèle...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    print(\"Application de LoRA pour l'entraînement efficace...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(**LORA_CONFIG)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    print(f\"Nombre de paramètres entraînables: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=TRAIN_CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=TRAIN_CONFIG[\"num_epochs\"],\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        warmup_steps=TRAIN_CONFIG[\"warmup_steps\"],\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Début de l'entraînement...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Sauvegarde du modèle...\")\n",
    "    model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "    print(\"\\nGénération d'exemples de texte:\")\n",
    "    for _ in range(3):\n",
    "        prompt = tokenizer.decode(tokenized_dataset[\"validation\"][np.random.randint(0, len(tokenized_dataset[\"validation\"]))][\"input_ids\"][:10])\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Texte généré: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18271c",
   "metadata": {},
   "source": [
    "## 4. Point d'entrée du script\n",
    "Cela permet une exécution contrôlée du programme, avec gestion d'interruptions utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ba885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset...\n",
      "Chargement du tokenizer...\n",
      "Prétraitement des données...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca58d659dcf4cc891363707db076f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378015dd0fbc4c6da4848abad6e26b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f59f7ec08a43aeb51414d985352756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du modèle...\n",
      "Application de LoRA pour l'entraînement efficace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres entraînables: 811008\n",
      "Début de l'entraînement...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='766' max='13770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  766/13770 04:23 < 1:14:39, 2.90 it/s, Epoch 0.17/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Entraînement interrompu par l'utilisateur.\")\n",
    "    finally:\n",
    "        print(\"Fin du programme.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
