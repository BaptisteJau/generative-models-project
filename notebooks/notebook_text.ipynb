{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c651616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Text Generation Pipeline with GPT2 on Wikitext-2 (Fixed Version)\n",
    "\n",
    "# # --- 1. Imports ---\n",
    "# import torch\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments, TrainerCallback\n",
    "# from datasets import Dataset\n",
    "# import logging\n",
    "\n",
    "# # --- 2. Setup ---\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# # --- 3. Load Pretrained Model and Tokenizer ---\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "\n",
    "# # Define a padding token (GPT-2 does not have one by default)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Move the model to the appropriate device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Enable gradient checkpointing to save memory\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "# # --- 4. Use Predefined Dataset ---\n",
    "# # Simulated high-quality text dataset to ensure stability\n",
    "# train_texts = [\n",
    "#     \"Once upon a time, in a land far away, there lived a wise old king.\",\n",
    "#     \"The quick brown fox jumps over the lazy dog.\",\n",
    "#     \"Artificial intelligence is transforming the world in many ways.\",\n",
    "#     \"The rain in Spain stays mainly in the plain.\",\n",
    "#     \"In the midst of chaos, there is also opportunity.\"\n",
    "# ] * 1000  # Repeat to simulate dataset size\n",
    "\n",
    "# train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "\n",
    "# # --- 5. Prepare Dataset ---\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer, mlm=False\n",
    "# )\n",
    "\n",
    "# # --- 6. Training Arguments ---\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./textgen_model\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     save_steps=500,\n",
    "#     save_total_limit=2,\n",
    "#     prediction_loss_only=True,\n",
    "#     logging_steps=100,\n",
    "#     fp16=torch.cuda.is_available(),\n",
    "#     report_to=\"none\",\n",
    "#     max_grad_norm=1.0\n",
    "# )\n",
    "\n",
    "# # --- 7. Trainer ---\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=tokenized_dataset\n",
    "# )\n",
    "\n",
    "# # Add a callback to log training loss\n",
    "# class LogCallback(TrainerCallback):\n",
    "#     def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "#         if logs is not None:\n",
    "#             logger.info(f\"Step {state.global_step}: {logs}\")\n",
    "\n",
    "# trainer.add_callback(LogCallback())\n",
    "\n",
    "# # --- 8. Train ---\n",
    "# trainer.train()\n",
    "\n",
    "# # --- 9. Test Text Generation ---\n",
    "# def generate_text(prompt, max_length=100):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "#     outputs = model.generate(\n",
    "#         **inputs,\n",
    "#         max_length=max_length,\n",
    "#         num_return_sequences=1,\n",
    "#         do_sample=True,\n",
    "#         top_k=50,\n",
    "#         top_p=0.95\n",
    "#     )\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# # --- 10. Example Usage ---\n",
    "# prompt = \"The future of artificial intelligence\"\n",
    "# print(\"\\n--- Generated Text ---\\n\")\n",
    "# print(generate_text(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d669d42",
   "metadata": {},
   "source": [
    "# Script d'entraînement GPT-2 avec LoRA sur Wikitext-2\n",
    "Ce notebook structure le script en cellules pour faciliter la compréhension et l'exécution.\n",
    "\n",
    "## 1. Importation des bibliothèques nécessaires\n",
    "On importe les modules essentiels pour le traitement des données, le modèle, l'entraînement et la gestion des configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a9811b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import notebook_login\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df67540",
   "metadata": {},
   "source": [
    "## 2. Définition des configurations\n",
    "On définit les noms des modèles, les chemins de sauvegarde, les hyperparamètres d'entraînement et les paramètres LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d093584",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilgpt2\"\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_CONFIG = {\n",
    "    \"batch_size\": 8,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"num_epochs\": 3,\n",
    "    \"max_length\": 128,\n",
    "    \"warmup_steps\": 100,\n",
    "}\n",
    "\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"target_modules\": [\"c_attn\", \"c_proj\"],\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871be77",
   "metadata": {},
   "source": [
    "## 3. Fonction principale\n",
    "Définition de `main()` pour organiser le flux d'exécution de bout en bout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c4b6839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 8, 'learning_rate': 0.0003, 'num_epochs': 3, 'max_length': 128, 'warmup_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_CONFIG)\n",
    "\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    accelerator = Accelerator()\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Chargement du dataset...\")\n",
    "    dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "\n",
    "    print(\"Chargement du tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(\"Prétraitement des données...\")\n",
    "    def preprocess_function(examples, tokenizer, max_length):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": TRAIN_CONFIG[\"max_length\"]},\n",
    "    )\n",
    "\n",
    "    print(\"Initialisation du modèle...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    print(\"Application de LoRA pour l'entraînement efficace...\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(**LORA_CONFIG)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    print(f\"Nombre de paramètres entraînables: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=TRAIN_CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=TRAIN_CONFIG[\"num_epochs\"],\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        warmup_steps=TRAIN_CONFIG[\"warmup_steps\"],\n",
    "        report_to=\"tensorboard\",\n",
    "        logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    print(\"Début de l'entraînement...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Sauvegarde du modèle...\")\n",
    "    model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "    print(\"\\nGénération d'exemples de texte:\")\n",
    "    for _ in range(3):\n",
    "        prompt = tokenizer.decode(tokenized_dataset[\"validation\"][np.random.randint(0, len(tokenized_dataset[\"validation\"]))][\"input_ids\"][:10])\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Texte généré: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e18271c",
   "metadata": {},
   "source": [
    "## 4. Point d'entrée du script\n",
    "Cela permet une exécution contrôlée du programme, avec gestion d'interruptions utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "143ba885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset...\n",
      "Chargement du tokenizer...\n",
      "Prétraitement des données...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca58d659dcf4cc891363707db076f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378015dd0fbc4c6da4848abad6e26b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f59f7ec08a43aeb51414d985352756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation du modèle...\n",
      "Application de LoRA pour l'entraînement efficace...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres entraînables: 811008\n",
      "Début de l'entraînement...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4591' max='13770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4591/13770 39:27 < 1:18:55, 1.94 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.639900</td>\n",
       "      <td>3.643584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin du programme.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.transformer.wte.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEntraînement interrompu par l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mutilisateur.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m trainer = Trainer(\n\u001b[32m     70\u001b[39m     model=model,\n\u001b[32m     71\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     data_collator=data_collator,\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDébut de l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mentraînement...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSauvegarde du modèle...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m model.save_pretrained(os.path.join(OUTPUT_DIR, \u001b[33m\"\u001b[39m\u001b[33mfinal_model\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2661\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2658\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2660\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2667\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3103\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3100\u001b[39m         \u001b[38;5;28mself\u001b[39m.control.should_save = is_new_best_metric\n\u001b[32m   3102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3103\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3104\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_save(\u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3200\u001b[39m, in \u001b[36mTrainer._save_checkpoint\u001b[39m\u001b[34m(self, model, trial)\u001b[39m\n\u001b[32m   3198\u001b[39m run_dir = \u001b[38;5;28mself\u001b[39m._get_output_dir(trial=trial)\n\u001b[32m   3199\u001b[39m output_dir = os.path.join(run_dir, checkpoint_folder)\n\u001b[32m-> \u001b[39m\u001b[32m3200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy \u001b[38;5;129;01min\u001b[39;00m [SaveStrategy.STEPS, SaveStrategy.EPOCH] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.best_global_step:\n\u001b[32m   3203\u001b[39m     best_checkpoint_folder = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPREFIX_CHECKPOINT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state.best_global_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3902\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3899\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3901\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3902\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3904\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:4000\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   3998\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3999\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_safetensors:\n\u001b[32m-> \u001b[39m\u001b[32m4000\u001b[39m     \u001b[43msafetensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4004\u001b[39m     torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\safetensors\\torch.py:488\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    485\u001b[39m         failing.append(names)\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m    491\u001b[39m \u001b[33m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[32m    492\u001b[39m \u001b[33m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[31mRuntimeError\u001b[39m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.lm_head.weight', 'base_model.model.transformer.wte.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Entraînement interrompu par l'utilisateur.\")\n",
    "    finally:\n",
    "        print(\"Fin du programme.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea15c446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle non trouvé à ./results\\final_model. Utilisation du modèle actif en mémoire.\n",
      "Calcul de la perplexité sur l'ensemble de validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef8b0f4ee5f4c559c7fe6773cc8e9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "cannot access free variable 'tokenizer' where it is not associated with a value in enclosing scope",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    153\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mperplexité\u001b[39m\u001b[33m'\u001b[39m: perplexity,\n\u001b[32m    154\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdonnées_génération\u001b[39m\u001b[33m'\u001b[39m: generation_df\n\u001b[32m    155\u001b[39m     }\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Exécuter la pipeline d'évaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m evaluation_results = \u001b[43mevaluate_model_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mevaluate_model_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    144\u001b[39m     plt.show()\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# Exécution de la pipeline complète\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m perplexity = \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m generation_df = generate_text_samples()\n\u001b[32m    149\u001b[39m analyze_embeddings()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mevaluate_model_pipeline.<locals>.calculate_perplexity\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Si l'ensemble n'est pas tokenisé, le faire maintenant\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eval_dataset.features:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     eval_dataset = \u001b[43meval_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAIN_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m model.eval()\n\u001b[32m     45\u001b[39m total_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3074\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[39m\n\u001b[32m   3068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3069\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3070\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3071\u001b[39m         total=pbar_total,\n\u001b[32m   3072\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3073\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3074\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3516\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[39m\n\u001b[32m   3514\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3515\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3516\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3518\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3466\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3466\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Hargalf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:3389\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3389\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3390\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mevaluate_model_pipeline.<locals>.calculate_perplexity.<locals>.<lambda>\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Si l'ensemble n'est pas tokenisé, le faire maintenant\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m eval_dataset.features:\n\u001b[32m     37\u001b[39m     eval_dataset = eval_dataset.map(\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m examples: \u001b[43mtokenizer\u001b[49m(examples[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     39\u001b[39m                                   truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, max_length=TRAIN_CONFIG[\u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     40\u001b[39m         batched=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     41\u001b[39m         remove_columns=[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     42\u001b[39m     )\n\u001b[32m     44\u001b[39m model.eval()\n\u001b[32m     45\u001b[39m total_loss = \u001b[32m0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: cannot access free variable 'tokenizer' where it is not associated with a value in enclosing scope"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def evaluate_model_pipeline():\n",
    "    \"\"\"\n",
    "    Pipeline d'évaluation complète pour le modèle GPT-2 avec LoRA entraîné sur Wikitext-2\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "    \n",
    "    # 1. Chargement du modèle entraîné (si nécessaire)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Modèle non trouvé à {model_path}. Utilisation du modèle actif en mémoire.\")\n",
    "    else:\n",
    "        print(f\"Chargement du modèle depuis {model_path}\")\n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        model_base = AutoModelForCausalLM.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(model_base, model_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # 2. Calcul de perplexité sur l'ensemble de validation\n",
    "    def calculate_perplexity():\n",
    "        print(\"Calcul de la perplexité sur l'ensemble de validation...\")\n",
    "        eval_dataset = raw_dataset[\"validation\"]\n",
    "        \n",
    "        # Si l'ensemble n'est pas tokenisé, le faire maintenant\n",
    "        if \"input_ids\" not in eval_dataset.features:\n",
    "            eval_dataset = eval_dataset.map(\n",
    "                lambda examples: tokenizer(examples[\"text\"], return_tensors=\"pt\", padding=\"max_length\", \n",
    "                                          truncation=True, max_length=TRAIN_CONFIG[\"max_length\"]),\n",
    "                batched=True,\n",
    "                remove_columns=[\"text\"]\n",
    "            )\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(0, min(len(eval_dataset), 100))):\n",
    "                inputs = {\n",
    "                    k: torch.tensor(v[i:i+1]).to(model.device) \n",
    "                    for k, v in eval_dataset.items() if k in [\"input_ids\", \"attention_mask\"]\n",
    "                }\n",
    "                \n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                total_loss += outputs.loss.item() * inputs[\"input_ids\"].shape[1]\n",
    "                total_tokens += inputs[\"input_ids\"].shape[1]\n",
    "        \n",
    "        perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "        print(f\"Perplexité: {perplexity.item():.2f}\")\n",
    "        return perplexity.item()\n",
    "    \n",
    "    # 3. Génération de texte avec différentes configurations\n",
    "    def generate_text_samples():\n",
    "        print(\"\\n--- Génération d'exemples avec différentes configurations ---\")\n",
    "        prompts = [\n",
    "            \"L'intelligence artificielle peut\",\n",
    "            \"Dans le futur, les ordinateurs\",\n",
    "            \"La recherche scientifique montre que\"\n",
    "        ]\n",
    "        \n",
    "        config_params = [\n",
    "            {\"temperature\": 0.7, \"top_p\": 0.9, \"config_name\": \"Standard\"},\n",
    "            {\"temperature\": 0.3, \"top_p\": 0.9, \"config_name\": \"Conservateur\"},\n",
    "            {\"temperature\": 1.2, \"top_p\": 0.9, \"config_name\": \"Créatif\"}\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for prompt in prompts[:2]:\n",
    "            print(f\"\\nPrompt: {prompt}\")\n",
    "            \n",
    "            for config in config_params:\n",
    "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=100,\n",
    "                    num_return_sequences=1,\n",
    "                    do_sample=True,\n",
    "                    temperature=config[\"temperature\"],\n",
    "                    top_p=config[\"top_p\"],\n",
    "                )\n",
    "                \n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"\\n{config['config_name']}:\\n{generated_text}\\n{'-'*50}\")\n",
    "                \n",
    "                results.append({\n",
    "                    'prompt': prompt,\n",
    "                    'configuration': config['config_name'],\n",
    "                    'temperature': config['temperature'],\n",
    "                    'generated_text': generated_text,\n",
    "                    'text_length': len(generated_text.split())\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    # 4. Analyse des embeddings du modèle\n",
    "    def analyze_embeddings():\n",
    "        print(\"\\n--- Analyse des embeddings du modèle ---\")\n",
    "        \n",
    "        # Extraction des vecteurs d'embedding pour certains mots\n",
    "        words = [\"intelligence\", \"artificielle\", \"réseau\", \"neurones\", \n",
    "                 \"apprentissage\", \"données\", \"modèle\", \"ordinateur\"]\n",
    "        \n",
    "        # Récupérer les embeddings de ces mots\n",
    "        word_ids = [tokenizer.encode(word, add_special_tokens=False)[0] for word in words]\n",
    "        embeddings = model.get_input_embeddings().weight[word_ids].detach().cpu().numpy()\n",
    "        \n",
    "        # Réduction de dimension avec PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_embeddings = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=100)\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), \n",
    "                        fontsize=12, ha='center')\n",
    "        \n",
    "        plt.title(\"Projection PCA des embeddings de mots-clés\")\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 5. Visualisation de la longueur des textes générés\n",
    "    def visualize_generation_stats(generation_df):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='configuration', y='text_length', hue='prompt', data=generation_df)\n",
    "        plt.title('Longueur des textes générés par configuration')\n",
    "        plt.xlabel('Configuration')\n",
    "        plt.ylabel('Nombre de mots')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Exécution de la pipeline complète\n",
    "    perplexity = calculate_perplexity()\n",
    "    generation_df = generate_text_samples()\n",
    "    analyze_embeddings()\n",
    "    visualize_generation_stats(generation_df)\n",
    "    \n",
    "    return {\n",
    "        'perplexité': perplexity,\n",
    "        'données_génération': generation_df\n",
    "    }\n",
    "\n",
    "# Exécuter la pipeline d'évaluation\n",
    "evaluation_results = evaluate_model_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
