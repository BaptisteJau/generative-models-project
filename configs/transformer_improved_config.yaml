# Configuration améliorée et testée pour le transformer
model_type: transformer
model_name: transformer_improved
type: text_generation
vocab_size: 50257  # Taille du vocabulaire GPT-2

# Architecture équilibrée pour performances/qualité
embedding_dim: 256
num_heads: 4
num_layers: 4
dropout_rate: 0.25  # Augmenté pour réduire le surapprentissage

# Paramètres d'entraînement efficaces
learning_rate: 0.0002
batch_size: 32
max_sequence_length: 128
num_epochs: 15  # Réduit pour éviter le surapprentissage
save_every: 2  # Plus fréquent pour capturer le meilleur modèle tôt

# Techniques d'optimisation
use_amp: true
gradient_accumulation_steps: 2
weight_decay: 0.01
warmup_steps: 1000

dataset_path: tiny_shakespeare
use_layer_norm: true  # Active les couches LayerNorm supplémentaires
norm_first: true  # Appliquer normalisation avant l'attention (Pre-LN architecture)