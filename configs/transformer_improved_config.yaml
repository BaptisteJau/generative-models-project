# Configuration améliorée et testée pour le transformer
model_type: transformer
vocab_size: 50257  # Taille du vocabulaire GPT-2

# Architecture équilibrée pour performances/qualité
embedding_dim: 256
num_heads: 4
num_layers: 4
dropout_rate: 0.1

# Paramètres d'entraînement efficaces
learning_rate: 0.0001
batch_size: 8
max_sequence_length: 128
num_epochs: 30
save_every: 5

# Techniques d'optimisation
use_amp: true
gradient_accumulation_steps: 8
weight_decay: 0.01
warmup_steps: 1000