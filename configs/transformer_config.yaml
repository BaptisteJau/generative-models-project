model_name: transformer
type: text_generation
embedding_dim: 512
num_heads: 8
num_layers: 6
dropout_rate: 0.1
max_sequence_length: 128
vocab_size: 30000
learning_rate: 0.0001
batch_size: 32
num_epochs: 20
optimizer: Adam
loss_function: CrossEntropyLoss
evaluation_metric: perplexity
save_model_path: ./models/transformer_model.pth
load_model_path: ./models/transformer_model.pth
logging_steps: 100
seed: 42