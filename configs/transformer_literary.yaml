model_name: transformer_literary
type: text_generation
embedding_dim: 256
num_heads: 4
num_layers: 4
dropout_rate: 0.15
max_sequence_length: 256
learning_rate: 0.0002
batch_size: 32
num_epochs: 40
gradient_accumulation_steps: 2
use_amp: true
save_every: 5
dataset_path: data/text/literary_corpus.txt