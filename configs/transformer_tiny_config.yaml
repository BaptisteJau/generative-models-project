# Configuration ultra-minimale pour le transformer
model_type: transformer
vocab_size: 10000  # Réduit

# Architecture minimale
embedding_dim: 64   # Très petit
num_heads: 1        # Minimum absolu
num_layers: 1       # Minimum absolu
dropout_rate: 0.1

# Paramètres d'entraînement
learning_rate: 0.0005
batch_size: 2       # Très petit batch
max_sequence_length: 16  # Séquences très courtes
num_epochs: 5
save_every: 1

# Optimisations mémoire
use_amp: true
gradient_accumulation_steps: 4
device: "cuda"