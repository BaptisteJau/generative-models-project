# Configuration allégée pour l'entraînement rapide d'un Transformer
model_name: transformer_mini
type: text_generation
embedding_dim: 128
num_heads: 2
num_layers: 2
dropout_rate: 0.1
max_sequence_length: 128
learning_rate: 0.0003
batch_size: 64
num_epochs: 30
gradient_accumulation_steps: 1
use_amp: false
save_every: 5
dataset_path: tiny_shakespeare