# Configuration ultra-légère pour le transformer
model_type: transformer
vocab_size: 30000  # Sera mis à jour automatiquement

# Architecture très réduite pour économiser la mémoire
embedding_dim: 128  # Considérablement réduit
num_heads: 2        # Minimum viable
num_layers: 2       # Minimum viable
dropout_rate: 0.1

# Paramètres d'entraînement
learning_rate: 0.0005
batch_size: 4       # Très petit batch
max_sequence_length: 32
num_epochs: 20
save_every: 5

# Optimisations mémoire
use_amp: true
gradient_accumulation_steps: 4
device: "cuda" # ou "cpu" si la mémoire CUDA reste un problème